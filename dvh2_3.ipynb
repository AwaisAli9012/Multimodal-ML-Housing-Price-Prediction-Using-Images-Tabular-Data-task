{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a8d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4f4a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousingDataset(Dataset):\n",
    "    def __init__(self,csv_file,img_dir,transform=None):\n",
    "        self.data=pd.read_csv(csv_file)\n",
    "        self.img_dir=img_dir\n",
    "        self.transform=transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_name=os.path.join(self.img_dir,self.data.iloc[idx]['image_name'])\n",
    "        image=Image.open(img_name).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image=self.transform(image)\n",
    "        tabular = self.data.iloc[idx].drop(['price', 'image_name']).values.astype('float32')\n",
    "        tabular = torch.tensor(tabular)\n",
    "        price=torch.tensor(self.data.iloc[idx]['price'],dtype=torch.float32)\n",
    "        return image,tabular,price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "506c166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a49176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HousingDataset('data/housing_data.csv', 'data/images/', transform=transform)\n",
    "train_size=int(0.8*len(dataset))\n",
    "val_size=len(dataset)-train_size\n",
    "train_dataset,val_dataset=torch.utils.data.random_split(dataset,[train_size,val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f85c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9584f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalHousingModel(nn.Module):\n",
    "    def __init__(self, tabular_input_size, cnn_feature_size=512, hidden_size=128):\n",
    "        super(MultimodalHousingModel, self).__init__()\n",
    "\n",
    "        # CNN backbone\n",
    "        self.cnn = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.cnn.fc = nn.Identity()  # Remove final layer → extract features\n",
    "\n",
    "        # Tabular MLP\n",
    "        self.tabular_mlp = nn.Sequential(\n",
    "            nn.Linear(tabular_input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fusion + Regression Head\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(cnn_feature_size + 32, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, tabular):\n",
    "        img_features = self.cnn(image)        # [B, 512]\n",
    "        tab_features = self.tabular_mlp(tabular)  # [B, 32]\n",
    "        combined = torch.cat([img_features, tab_features], dim=1)  # [B, 544]\n",
    "        output = self.fusion(combined)\n",
    "        return output.squeeze(1)  # [B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a7f98bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\Abdullah/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')  # You're on CPU\n",
    "model = MultimodalHousingModel(tabular_input_size=5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a01462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Smart optimizer\n",
    "criterion = nn.MSELoss()  # Loss function for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2d0aac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 335961666355.2000\n",
      "Validation MAE: 556942.96, RMSE: 607519.21\n",
      "Epoch 2/50, Loss: 335955836928.0000\n",
      "Validation MAE: 556938.16, RMSE: 607514.27\n",
      "Epoch 3/50, Loss: 335950177894.4000\n",
      "Validation MAE: 556933.24, RMSE: 607509.19\n",
      "Epoch 4/50, Loss: 335944318976.0000\n",
      "Validation MAE: 556928.15, RMSE: 607503.99\n",
      "Epoch 5/50, Loss: 335938496102.4000\n",
      "Validation MAE: 556922.95, RMSE: 607498.66\n",
      "Epoch 6/50, Loss: 335932263628.8000\n",
      "Validation MAE: 556918.22, RMSE: 607493.68\n",
      "Epoch 7/50, Loss: 335925647769.6000\n",
      "Validation MAE: 556912.49, RMSE: 607487.87\n",
      "Epoch 8/50, Loss: 335918986035.2000\n",
      "Validation MAE: 556906.15, RMSE: 607481.42\n",
      "Epoch 9/50, Loss: 335911957299.2000\n",
      "Validation MAE: 556899.80, RMSE: 607474.84\n",
      "Epoch 10/50, Loss: 335904171622.4000\n",
      "Validation MAE: 556893.84, RMSE: 607468.49\n",
      "Epoch 11/50, Loss: 335895992729.6000\n",
      "Validation MAE: 556886.96, RMSE: 607461.32\n",
      "Epoch 12/50, Loss: 335886883225.6000\n",
      "Validation MAE: 556879.22, RMSE: 607453.43\n",
      "Epoch 13/50, Loss: 335878062080.0000\n",
      "Validation MAE: 556870.38, RMSE: 607444.59\n",
      "Epoch 14/50, Loss: 335867376435.2000\n",
      "Validation MAE: 556861.98, RMSE: 607435.74\n",
      "Epoch 15/50, Loss: 335857182310.4000\n",
      "Validation MAE: 556852.52, RMSE: 607426.09\n",
      "Epoch 16/50, Loss: 335845359616.0000\n",
      "Validation MAE: 556841.67, RMSE: 607415.15\n",
      "Epoch 17/50, Loss: 335833612288.0000\n",
      "Validation MAE: 556831.15, RMSE: 607404.15\n",
      "Epoch 18/50, Loss: 335819813683.2000\n",
      "Validation MAE: 556819.48, RMSE: 607392.11\n",
      "Epoch 19/50, Loss: 335806955520.0000\n",
      "Validation MAE: 556806.36, RMSE: 607378.65\n",
      "Epoch 20/50, Loss: 335790722252.8000\n",
      "Validation MAE: 556793.35, RMSE: 607365.03\n",
      "Epoch 21/50, Loss: 335774135091.2000\n",
      "Validation MAE: 556779.60, RMSE: 607350.79\n",
      "Epoch 22/50, Loss: 335757171097.6000\n",
      "Validation MAE: 556764.31, RMSE: 607334.95\n",
      "Epoch 23/50, Loss: 335737919897.6000\n",
      "Validation MAE: 556747.61, RMSE: 607317.77\n",
      "Epoch 24/50, Loss: 335718554009.6000\n",
      "Validation MAE: 556729.67, RMSE: 607299.14\n",
      "Epoch 25/50, Loss: 335698028134.4000\n",
      "Validation MAE: 556710.54, RMSE: 607279.59\n",
      "Epoch 26/50, Loss: 335675752448.0000\n",
      "Validation MAE: 556690.90, RMSE: 607259.05\n",
      "Epoch 27/50, Loss: 335650462105.6000\n",
      "Validation MAE: 556669.86, RMSE: 607237.15\n",
      "Epoch 28/50, Loss: 335625122611.2000\n",
      "Validation MAE: 556646.21, RMSE: 607212.85\n",
      "Epoch 29/50, Loss: 335595991859.2000\n",
      "Validation MAE: 556620.60, RMSE: 607186.69\n",
      "Epoch 30/50, Loss: 335569990451.2000\n",
      "Validation MAE: 556595.72, RMSE: 607159.99\n",
      "Epoch 31/50, Loss: 335536603136.0000\n",
      "Validation MAE: 556567.41, RMSE: 607130.15\n",
      "Epoch 32/50, Loss: 335505050828.8000\n",
      "Validation MAE: 556538.97, RMSE: 607100.87\n",
      "Epoch 33/50, Loss: 335472220569.6000\n",
      "Validation MAE: 556506.76, RMSE: 607068.10\n",
      "Epoch 34/50, Loss: 335432115814.4000\n",
      "Validation MAE: 556476.48, RMSE: 607035.67\n",
      "Epoch 35/50, Loss: 335394471936.0000\n",
      "Validation MAE: 556444.14, RMSE: 607001.62\n",
      "Epoch 36/50, Loss: 335355281408.0000\n",
      "Validation MAE: 556407.62, RMSE: 606963.39\n",
      "Epoch 37/50, Loss: 335308655820.8000\n",
      "Validation MAE: 556369.44, RMSE: 606924.96\n",
      "Epoch 38/50, Loss: 335265637990.4000\n",
      "Validation MAE: 556332.26, RMSE: 606886.01\n",
      "Epoch 39/50, Loss: 335218225971.2000\n",
      "Validation MAE: 556292.40, RMSE: 606843.00\n",
      "Epoch 40/50, Loss: 335168130252.8000\n",
      "Validation MAE: 556248.78, RMSE: 606797.44\n",
      "Epoch 41/50, Loss: 335115934105.6000\n",
      "Validation MAE: 556201.53, RMSE: 606748.31\n",
      "Epoch 42/50, Loss: 335059786137.6000\n",
      "Validation MAE: 556151.67, RMSE: 606697.07\n",
      "Epoch 43/50, Loss: 334999977984.0000\n",
      "Validation MAE: 556100.56, RMSE: 606644.31\n",
      "Epoch 44/50, Loss: 334939088486.4000\n",
      "Validation MAE: 556046.73, RMSE: 606588.77\n",
      "Epoch 45/50, Loss: 334879244288.0000\n",
      "Validation MAE: 555988.25, RMSE: 606528.40\n",
      "Epoch 46/50, Loss: 334809366528.0000\n",
      "Validation MAE: 555931.22, RMSE: 606468.97\n",
      "Epoch 47/50, Loss: 334738938265.6000\n",
      "Validation MAE: 555869.99, RMSE: 606405.07\n",
      "Epoch 48/50, Loss: 334667772723.2000\n",
      "Validation MAE: 555808.65, RMSE: 606337.30\n",
      "Epoch 49/50, Loss: 334592789708.8000\n",
      "Validation MAE: 555744.56, RMSE: 606272.21\n",
      "Epoch 50/50, Loss: 334510853324.8000\n",
      "Validation MAE: 555680.00, RMSE: 606204.69\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ➤ TRAINING PHASE\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, tabular, prices in train_loader:\n",
    "        images, tabular, prices = images.to(device), tabular.to(device), prices.to(device)\n",
    "\n",
    "        optimizer.zero_grad()           # Clear old gradients\n",
    "        outputs = model(images, tabular) # Robot makes predictions\n",
    "        loss = criterion(outputs, prices) # How wrong was it?\n",
    "        loss.backward()                 # Calculate gradients\n",
    "        optimizer.step()                # Update weights\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # ➤ VALIDATION PHASE\n",
    "    model.eval()\n",
    "    val_preds, val_targets = [], []\n",
    "    with torch.no_grad():  # No gradient calculation → faster + less memory\n",
    "        for images, tabular, prices in val_loader:\n",
    "            images, tabular, prices = images.to(device), tabular.to(device), prices.to(device)\n",
    "            outputs = model(images, tabular)\n",
    "            val_preds.extend(outputs.cpu().numpy())\n",
    "            val_targets.extend(prices.cpu().numpy())\n",
    "\n",
    "    mae = mean_absolute_error(val_targets, val_preds)\n",
    "    rmse = np.sqrt(mean_squared_error(val_targets, val_preds))\n",
    "    print(f\"Validation MAE: {mae:.2f}, RMSE: {rmse:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
